{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0a204-a270-4c70-9572-9f5492c9908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Load and Structure Your Data\n",
    "\n",
    "import pandas as pd\n",
    "from ragas import SingleTurnSample, evaluate\n",
    "from ragas.metrics import (\n",
    "    context_relevancy,\n",
    "    answer_relevancy,\n",
    "    faithfulness\n",
    ")\n",
    "from datasets import Dataset\n",
    "import ast\n",
    "\n",
    "def load_all_evaluation_results():\n",
    "    \"\"\"Load and combine all evaluation CSV files\"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    for i in range(1, 10):  # evaluation_results1.csv to evaluation_results9.csv\n",
    "        try:\n",
    "            df = pd.read_csv(f\"evaluation_results/evaluation_results{i}.csv\")\n",
    "            all_results.append(df)\n",
    "            print(f\"Loaded evaluation_results{i}.csv with {len(df)} rows\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File evaluation_results{i}.csv not found, skipping...\")\n",
    "\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(f\"Total combined results: {len(combined_df)} rows\")\n",
    "    return combined_df\n",
    "\n",
    "def convert_enhanced_results_to_ragas(results_df):\n",
    "    \"\"\"Convert enhanced results with actual retrieved context to RAGAS format\"\"\"\n",
    "    samples = []\n",
    "\n",
    "    for _, row in results_df.iterrows():\n",
    "        # Parse the retrieved context\n",
    "        retrieved_context = row.get('retrieved_context', '[]')\n",
    "\n",
    "        if row['method'] in ['bm25', 'vector', 'hybrid']:\n",
    "            # Parse the saved context chunks\n",
    "            try:\n",
    "                contexts = eval(retrieved_context) if isinstance(retrieved_context, str) else [retrieved_context]\n",
    "                if not isinstance(contexts, list):\n",
    "                    contexts = [str(contexts)]\n",
    "            except:\n",
    "                contexts = [str(retrieved_context)]\n",
    "        else:\n",
    "            # For graph/context stuffing methods\n",
    "            contexts = [str(retrieved_context)]\n",
    "\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=row['question'],\n",
    "            response=row['answer'],\n",
    "            retrieved_contexts=contexts,\n",
    "            metadata={\n",
    "                'method': row['method'],\n",
    "                'response_time': row.get('response_time', 0),\n",
    "                'context_chunks_count': row.get('context_chunks_count', 0),\n",
    "                'citations': row.get('citations', [])\n",
    "            }\n",
    "        )\n",
    "        samples.append(sample)\n",
    "\n",
    "    return samples\n",
    "\n",
    "### 3. Load Data and Convert\n",
    "print(\"Loading evaluation results...\")\n",
    "results_df = load_all_evaluation_results()\n",
    "\n",
    "print(\"Converting to RAGAS format...\")\n",
    "samples = convert_enhanced_results_to_ragas(results_df)\n",
    "print(f\"Created {len(samples)} RAGAS samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea42ee-85fd-4d50-831f-5cb62fbef8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Run RAGAS Evaluation\n",
    "\n",
    "methods = ['bm25', 'vector', 'hybrid', 'graph_only', 'context_stuffing', 'graph_stuffing']\n",
    "evaluation_results = {}\n",
    "\n",
    "for method in methods:\n",
    "    method_samples = [s for s in samples if s.metadata['method'] == method]\n",
    "\n",
    "    if not method_samples:\n",
    "        print(f\"No samples found for method: {method}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nEvaluating {method} method with {len(method_samples)} samples...\")\n",
    "\n",
    "    try:\n",
    "        # Convert to HuggingFace Dataset format\n",
    "        data_dict = {\n",
    "            'question': [s.user_input for s in method_samples],\n",
    "            'answer': [s.response for s in method_samples],\n",
    "            'contexts': [s.retrieved_contexts for s in method_samples]\n",
    "        }\n",
    "\n",
    "        dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "        # Run RAGAS evaluation\n",
    "        result = evaluate(\n",
    "            dataset=dataset,\n",
    "            metrics=[\n",
    "                context_relevancy,    # How relevant is retrieved context?\n",
    "                answer_relevancy,     # How relevant is answer to question?\n",
    "                faithfulness          # Does answer contradict context?\n",
    "                # Note: context_recall removed - requires ground truth\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        evaluation_results[method] = result\n",
    "        print(f\"‚úÖ {method} evaluation complete\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {method} evaluation failed: {e}\")\n",
    "\n",
    "### 5. Display Results\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä RAGAS EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method, result in evaluation_results.items():\n",
    "    print(f\"\\nüîç {method.upper()}:\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    if isinstance(result, dict):\n",
    "        for metric_name, score in result.items():\n",
    "            print(f\"  {metric_name:20}: {score:.3f}\")\n",
    "    else:\n",
    "        print(f\"  Result: {result}\")\n",
    "\n",
    "### 6. Save Results Summary\n",
    "\n",
    "summary_data = []\n",
    "for method, result in evaluation_results.items():\n",
    "    if isinstance(result, dict):\n",
    "        row = {'method': method}\n",
    "        row.update(result)\n",
    "        summary_data.append(row)\n",
    "\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(\"ragas_evaluation_summary.csv\", index=False)\n",
    "    print(f\"\\n‚úÖ Results saved to ragas_evaluation_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a78d62-858d-40f8-8dea-648ee06fe069",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7. Optional: Legal-Specific Custom Metric (Advanced)\n",
    "\n",
    "from ragas.metrics.base import MetricWithLLM\n",
    "\n",
    "class LegalAccuracy(MetricWithLLM):\n",
    "    name = \"legal_accuracy\"\n",
    "\n",
    "    def _compute_score(self, row):\n",
    "        prompt = f\"\"\"\n",
    "        Question: {row['question']}\n",
    "        Answer: {row['answer']}\n",
    "        Context: {row['contexts']}\n",
    "\n",
    "        As a legal expert, rate the accuracy of this answer (1-10):\n",
    "        - Are legal facts correctly stated?\n",
    "        - Are citations properly supported by evidence?\n",
    "        - Is the legal reasoning sound?\n",
    "        - Are conclusions appropriately qualified?\n",
    "\n",
    "        Return only a number from 1-10.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.llm.generate(prompt)\n",
    "            score = float(response.strip()) / 10.0  # Normalize to 0-1\n",
    "            return max(0.0, min(1.0, score))  # Clamp to [0,1]\n",
    "        except:\n",
    "            return 0.5  # Default score if parsing fails\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
