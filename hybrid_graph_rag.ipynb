{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hybrid_overview",
   "metadata": {},
   "source": [
    "# Hybrid Graph-RAG for DPP PoC\n",
    "\n",
    "This notebook combines:\n",
    "- Full-text search (BM25) for evidence retrieval\n",
    "- Vector semantic search using pre-computed embeddings\n",
    "- Hybrid search combining both methods\n",
    "- Graph neighborhood expansion for context\n",
    "- LLM synthesis with proper citations\n",
    "\n",
    "**Prerequisites**: \n",
    "1. Run `vector-embeddings.ipynb` first to create chunks, embeddings, and Neo4j indexes\n",
    "2. Ensure Neo4j database contains the knowledge graph entities from the charge bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "setup_imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "‚úÖ Downloaded model from HuggingFace\n",
      "‚úÖ Connected to Neo4j\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import json, textwrap, re, os\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env from home directory in AzureML\n",
    "home_env = Path.home() / '.env'\n",
    "if home_env.exists():\n",
    "    load_dotenv(home_env)\n",
    "else:\n",
    "    load_dotenv()  # fallback to local .env\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USER = os.getenv('NEO4J_USER')\n",
    "NEO4J_PASS = os.getenv('NEO4J_PASS')\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "EMBED_DIM = 384\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\" \n",
    "\n",
    "# Initialize embedding model for query-time vector search\n",
    "# Note: Vector embeddings for chunks should already exist from vector-embeddings.ipynb\n",
    "print(\"Loading embedding model...\")\n",
    "try:\n",
    "    # Try local cached model first\n",
    "    embedding_model = SentenceTransformer(\"/Users/alandevlin/data/all-MiniLM-L6-v2\")\n",
    "    print(\"‚úÖ Loaded local cached model\")\n",
    "except:\n",
    "    # Fallback to download (for AzureML)\n",
    "    embedding_model = SentenceTransformer(EMBED_MODEL)\n",
    "    print(\"‚úÖ Downloaded model from HuggingFace\")\n",
    "\n",
    "print(\"‚úÖ Connected to Neo4j\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrieval_section",
   "metadata": {},
   "source": [
    "## Evidence Retrieval System\n",
    "\n",
    "Supports both BM25 full-text search and vector semantic search for evidence retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "setup_fulltext",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Full-text index ready\n"
     ]
    }
   ],
   "source": [
    "# Create full-text index for BM25 search\n",
    "with driver.session(database=\"dpppoc\") as s:\n",
    "    try:\n",
    "        s.run(\"\"\"\n",
    "          CREATE FULLTEXT INDEX chunk_fulltext IF NOT EXISTS \n",
    "          FOR (c:Chunk) ON EACH [c.text, c.source_file, c.exhibit_id]\n",
    "        \"\"\")\n",
    "        print(\"‚úÖ Full-text index ready\")\n",
    "    except Exception as e:\n",
    "        print(\"Index already exists:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "retrieval_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_chunks(query, k=8):\n",
    "    \"\"\"Search chunks using BM25 full-text search.\"\"\"\n",
    "    with driver.session(database=\"dpppoc\") as s:\n",
    "        return s.run(\"\"\"\n",
    "            CALL db.index.fulltext.queryNodes('chunk_fulltext', $q)\n",
    "            YIELD node AS c, score\n",
    "            RETURN c.id AS id, c.exhibit_id AS ex, c.source_file AS file,\n",
    "                   c.text AS text, score\n",
    "            ORDER BY score DESC LIMIT $k\n",
    "        \"\"\", q=query, k=k, timeout=15).data()\n",
    "\n",
    "def vector_chunks(query, k=8):\n",
    "    \"\"\"Search chunks using vector similarity search.\"\"\"\n",
    "    if embedding_model is None:\n",
    "        print(\"Vector search not available - falling back to BM25\")\n",
    "        return bm25_chunks(query, k)\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_vector = embedding_model.encode(query, show_progress_bar=False).tolist()\n",
    "    \n",
    "    with driver.session(database=\"dpppoc\") as s:\n",
    "        return s.run(\"\"\"\n",
    "            CALL db.index.vector.queryNodes('chunk_vec', $k, $embedding)\n",
    "            YIELD node AS c, score\n",
    "            RETURN c.id AS id, c.exhibit_id AS ex, c.source_file AS file,\n",
    "                   c.text AS text, score\n",
    "            ORDER BY score DESC\n",
    "        \"\"\", k=k, embedding=query_vector, timeout=15).data()\n",
    "\n",
    "def hybrid_chunks(query, k=8, vector_weight=0.7):\n",
    "    \"\"\"Combine BM25 and vector search results with weighted scores.\"\"\"\n",
    "    if embedding_model is None:\n",
    "        return bm25_chunks(query, k)\n",
    "    \n",
    "    # Get results from both methods\n",
    "    bm25_results = bm25_chunks(query, k*2)\n",
    "    vector_results = vector_chunks(query, k*2)\n",
    "    \n",
    "    # Normalize scores and combine\n",
    "    combined = {}\n",
    "    \n",
    "    # Add BM25 results\n",
    "    if bm25_results:\n",
    "        max_bm25 = max(r['score'] for r in bm25_results)\n",
    "        for r in bm25_results:\n",
    "            chunk_id = r['id']\n",
    "            norm_score = r['score'] / max_bm25 if max_bm25 > 0 else 0\n",
    "            combined[chunk_id] = {\n",
    "                **r,\n",
    "                'score': norm_score * (1 - vector_weight)\n",
    "            }\n",
    "    \n",
    "    # Add vector results\n",
    "    if vector_results:\n",
    "        max_vector = max(r['score'] for r in vector_results)\n",
    "        for r in vector_results:\n",
    "            chunk_id = r['id']\n",
    "            norm_score = r['score'] / max_vector if max_vector > 0 else 0\n",
    "            if chunk_id in combined:\n",
    "                combined[chunk_id]['score'] += norm_score * vector_weight\n",
    "            else:\n",
    "                combined[chunk_id] = {\n",
    "                    **r,\n",
    "                    'score': norm_score * vector_weight\n",
    "                }\n",
    "    \n",
    "    # Sort by combined score\n",
    "    return sorted(combined.values(), key=lambda x: x['score'], reverse=True)[:k]\n",
    "\n",
    "def dedupe_chunks(rows):\n",
    "    \"\"\"Remove duplicate chunks by ID.\"\"\"\n",
    "    seen, out = set(), []\n",
    "    for r in rows:\n",
    "        if r[\"id\"] not in seen:\n",
    "            out.append(r); seen.add(r[\"id\"])\n",
    "    return out\n",
    "\n",
    "def rerank_by_exhibit(chunks, preferred=None, boost=0.12):\n",
    "    \"\"\"Boost scores for preferred exhibits.\"\"\"\n",
    "    if not preferred: return chunks\n",
    "    preferred = set(preferred)\n",
    "    boosted = []\n",
    "    for c in chunks:\n",
    "        c2 = dict(c)\n",
    "        c2[\"score\"] = float(c[\"score\"]) + (boost if c[\"ex\"] in preferred else 0.0)\n",
    "        boosted.append(c2)\n",
    "    return sorted(boosted, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "def retrieve_chunks(question, k=8, preferred_exhibits=None, method=\"bm25\"):\n",
    "    \"\"\"Main retrieval function with multiple search methods.\n",
    "    \n",
    "    Args:\n",
    "        question: Query string\n",
    "        k: Number of chunks to return\n",
    "        preferred_exhibits: List of exhibit IDs to boost\n",
    "        method: 'bm25', 'vector', or 'hybrid'\n",
    "    \"\"\"\n",
    "    # Choose search method\n",
    "    if method == \"vector\":\n",
    "        rows = vector_chunks(question, k=20)\n",
    "    elif method == \"hybrid\":\n",
    "        rows = hybrid_chunks(question, k=20)\n",
    "    else:  # default to bm25\n",
    "        rows = bm25_chunks(question, k=20)\n",
    "    \n",
    "    # Post-process results\n",
    "    rows = dedupe_chunks(rows)\n",
    "    rows = rerank_by_exhibit(rows, preferred_exhibits)\n",
    "    return rows[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph_expansion",
   "metadata": {},
   "source": [
    "## Graph Context Expansion\n",
    "\n",
    "Expands retrieved chunks with related graph entities for additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "graph_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_neighborhood(chunk_ids, max_nodes=50):\n",
    "    \"\"\"Get graph context for retrieved chunks.\"\"\"\n",
    "    with driver.session(database=\"dpppoc\") as s:\n",
    "        return s.run(\"\"\"\n",
    "            UNWIND $ids AS cid\n",
    "            MATCH (c:Chunk {id:cid})-[:FROM_EXHIBIT]->(e:Exhibit)\n",
    "            OPTIONAL MATCH (c)-[:MENTIONS]->(n)\n",
    "            RETURN c.id AS id, e.id AS exhibit,\n",
    "                   collect(DISTINCT {labels:labels(n), props:n{.*}})[0..$m] AS nodes\n",
    "        \"\"\", ids=chunk_ids, m=max_nodes).data()\n",
    "\n",
    "def format_evidence(chunks, max_chars=800):\n",
    "    \"\"\"Format evidence chunks for LLM prompt.\"\"\"\n",
    "    blocks = []\n",
    "    for ch in chunks[:8]:\n",
    "        blocks.append(\n",
    "            f\"[Exhibit {ch['ex']}, Chunk {ch['id']}]\\n\" +\n",
    "            textwrap.shorten(ch[\"text\"], max_chars)\n",
    "        )\n",
    "    return \"\\n\\n\".join(blocks) if blocks else \"(none)\"\n",
    "\n",
    "def format_graph_facts(neighborhoods, max_items=3):\n",
    "    \"\"\"Format graph context for LLM prompt.\"\"\"\n",
    "    return json.dumps(neighborhoods[:max_items], indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "answer_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, preferred_exhibits=None, k=8, method=\"bm25\") -> Dict:\n",
    "    \"\"\"Main function for answering questions using hybrid graph-RAG.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        preferred_exhibits: List of exhibit IDs to boost (e.g., ['E7', 'E5'])\n",
    "        k: Number of chunks to retrieve\n",
    "        method: Retrieval method - 'bm25', 'vector', or 'hybrid'\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'answer', 'citations', and metadata\n",
    "    \"\"\"\n",
    "    # Retrieve relevant chunks using specified method\n",
    "    chunks = retrieve_chunks(question, k=k, preferred_exhibits=preferred_exhibits, method=method)\n",
    "    \n",
    "    # Expand with graph context\n",
    "    neighborhoods = expand_neighborhood([c[\"id\"] for c in chunks[:8]])\n",
    "    \n",
    "    answer = synthesize_answer(question, chunks, neighborhoods)\n",
    "    # Generate grounded answer\n",
    "    \n",
    "    # Extract citations\n",
    "    citations = sorted(list({c['ex'] for c in chunks}))\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"citations\": citations,\n",
    "        \"method\": method,\n",
    "        \"chunks_used\": len(chunks),\n",
    "        \"graph_entities\": sum(len(n.get('nodes', [])) for n in neighborhoods)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm_synthesis",
   "metadata": {},
   "source": [
    "## LLM Synthesis with Citations\n",
    "\n",
    "Combines retrieved evidence and graph context to generate grounded answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "openai_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI client configured\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_INSTRUCTIONS = \"\"\"You are assisting the DPP with charge brief QA.\n",
    "Answer ONLY using the EVIDENCE and GRAPH FACTS provided.\n",
    "Every factual sentence MUST end with a citation like [Exhibit {ex}, Chunk {id}].\n",
    "If evidence is insufficient, explicitly say what exhibits/witnesses to check next.\n",
    "Be concise, formal, and neutral; avoid speculation beyond the evidence.\"\"\"\n",
    "\n",
    "# Setup OpenAI client\n",
    "OpenAI = None\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    try:\n",
    "        from openai import OpenAI as _OpenAI\n",
    "        OpenAI = _OpenAI()\n",
    "        print(\"‚úÖ OpenAI client configured\")\n",
    "    except Exception as e:\n",
    "        print(\"OpenAI client not available:\", e)\n",
    "\n",
    "def _call_openai(messages, model=\"gpt-4o-mini\", temperature=0.2):\n",
    "    \"\"\"Call OpenAI API with messages.\"\"\"\n",
    "    if OpenAI is None:\n",
    "        raise RuntimeError(\"OpenAI client not configured\")\n",
    "    resp = OpenAI.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=messages\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "def synthesize_answer(question: str, chunks: List[Dict], neighborhoods: List[Dict],\n",
    "                      model=\"gpt-4o-mini\") -> str:\n",
    "    \"\"\"Generate grounded answer using OpenAI with proper citations.\"\"\"\n",
    "    evidence = format_evidence(chunks)\n",
    "    graphfacts = format_graph_facts(neighborhoods)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_INSTRUCTIONS},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"EVIDENCE:\n",
    "{evidence}\n",
    "\n",
    "GRAPH FACTS:\n",
    "{graphfacts}\n",
    "\n",
    "QUESTION:\n",
    "{question}\"\"\"}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        txt = _call_openai(messages, model=model)\n",
    "    except Exception as e:\n",
    "        print(f\"LLM error: {e}\")\n",
    "        # Fallback using top chunk\n",
    "        top = chunks[0] if chunks else None\n",
    "        return \"(fallback) Insufficient LLM availability.\" if not top else \\\n",
    "               f\"(fallback) {top['text'][:140]}... [Exhibit {top['ex']}, Chunk {top['id']}]\"\n",
    "\n",
    "    # Ensure at least one citation\n",
    "    if not re.search(r\"\\[Exhibit\\s+E\\d+,\\s*Chunk\\s+[0-9a-f-]+\\]\", txt, re.I):\n",
    "        if chunks:\n",
    "            txt += f\" [Exhibit {chunks[0]['ex']}, Chunk {chunks[0]['id']}]\"\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "example_usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç QUESTION: What are some suspicious findings in the evidence?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìä BM25 METHOD:\n",
      "----------------------------------------\n",
      "ANSWER: The evidence presents several suspicious findings:\n",
      "\n",
      "1. The vehicle belonging to Dr. Valerie Somers, a blue Honda Civic, was found near Lantana Park, which raises questions about her disappearance and potential foul play [Exhibit E0, Chunk a8b83294-0944-4260-95cc-11f95ecf432f].\n",
      "\n",
      "2. A partial latent fingerprint matching Patrick Phelan was found on the door frame of the recovered vehicle, indicating his presence at the scene [Exhibit E4, Chunk edfa2ecf-8540-4840-8dbf-8944bcb0bf8a].\n",
      "\n",
      "3. Trace fibres consistent with a scarf worn by the victim were discovered in the boot lining of the vehicle, suggesting a connection between the victim and the accused [Exhibit E4, Chunk edfa2ecf-8540-4840-8dbf-8944bcb0bf8a].\n",
      "\n",
      "4. Patrick Phelan initially denied contact with Dr. Somers on the day of her disappearance but later admitted to meeting her to \"settle differences,\" which raises suspicion about his involvement [Exhibit E5, Chunk bd880a09-fa2a-4465-8e9f-b1ebec9eb6e9].\n",
      "\n",
      "5. The autopsy revealed that Dr. Somers died from blunt force trauma to the head, consistent with an assault, which contradicts Phelan's claim that her injuries were accidental [Exhibit E0, Chunk 964a120e-c849-4146-9566-f85934641217].\n",
      "\n",
      "These findings collectively suggest a strong circumstantial case against Patrick Phelan, warranting further investigation into his actions and motives surrounding the incident.\n",
      "CITATIONS: E0, E4, E5\n",
      "STATS: 8 chunks, 8 graph entities\n",
      "\n",
      "üìä VECTOR METHOD:\n",
      "----------------------------------------\n",
      "ANSWER: Suspicious findings in the evidence include:\n",
      "\n",
      "1. The presence of fibres matching Dr. Somers‚Äô scarf in the boot of Patrick Phelan‚Äôs vehicle, indicating a possible connection to the victim [Exhibit E0, Chunk 0a24172b-3dc8-4064-aca6-b3ba0dcceea6].\n",
      "2. Soil from Lantana Park matching that found in Phelan's vehicle, suggesting he was at the scene [Exhibit E0, Chunk 0a24172b-3dc8-4064-aca6-b3ba0dcceea6].\n",
      "3. A fingerprint found on the car door, potentially linking Phelan to the vehicle [Exhibit E0, Chunk 0a24172b-3dc8-4064-aca6-b3ba0dcceea6].\n",
      "4. Phelan expressed animosity towards Dr. Somers, stating that she was \"ruining\" his marriage, which provides a motive for potential harm [Exhibit E0, Chunk 127efdf9-4e65-4cda-9dfc-63fa7e505c54].\n",
      "5. The autopsy report refuted Phelan's claim of accidental injury, suggesting foul play [Exhibit E0, Chunk 127efdf9-4e65-4cda-9dfc-63fa7e505c54].\n",
      "\n",
      "For further verification, check the autopsy report (Exhibit E2) and the interview recording with Patrick Phelan (Exhibit E5) for additional context on his statements and behavior.\n",
      "CITATIONS: E0, E1, E6\n",
      "STATS: 8 chunks, 8 graph entities\n",
      "\n",
      "üìä HYBRID METHOD:\n",
      "----------------------------------------\n",
      "ANSWER: Several suspicious findings are present in the evidence:\n",
      "\n",
      "1. The vehicle belonging to Dr. Valerie Somers was found near Lantana Park, where her body was later discovered, indicating a potential connection to the crime scene [Exhibit E0, Chunk a8b83294-0944-4260-95cc-11f95ecf432f].\n",
      "\n",
      "2. A partial latent fingerprint matching Patrick Phelan was found on the door frame of Dr. Somers' vehicle [Exhibit E4, Chunk edfa2ecf-8540-4840-8dbf-8944bcb0bf8a].\n",
      "\n",
      "3. Trace fibres consistent with a scarf worn by Dr. Somers were located in the boot lining of Patrick Phelan's vehicle [Exhibit E4, Chunk edfa2ecf-8540-4840-8dbf-8944bcb0bf8a].\n",
      "\n",
      "4. Patrick Phelan initially denied contact with Dr. Somers but later admitted to meeting her to \"settle differences,\" which raises questions about his credibility [Exhibit E5, Chunk bd880a09-fa2a-4465-8b1ebec9eb6e9].\n",
      "\n",
      "5. The time of death was estimated to be between 19:30 and 20:15 hours on 23 November, which coincides with the timeline of Phelan's admission of an argument with Dr. Somers [Exhibit E0, Chunk 4].\n",
      "\n",
      "These findings collectively suggest a strong circumstantial case against Patrick Phelan, warranting further investigation into his actions during the relevant time period.\n",
      "CITATIONS: E0, E4, E5\n",
      "STATS: 8 chunks, 8 graph entities\n"
     ]
    }
   ],
   "source": [
    "# Compare different retrieval methods on the same question\n",
    "question = \"What are some suspicious findings in the evidence?\"\n",
    "\n",
    "print(f\"üîç QUESTION: {question}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "methods = [\"bm25\", \"vector\", \"hybrid\"]\n",
    "for method in methods:\n",
    "    print(f\"\\nüìä {method.upper()} METHOD:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        result = answer_question(question, method=method)\n",
    "        \n",
    "        print(f\"ANSWER: {result['answer']}\")\n",
    "        print(f\"CITATIONS: {', '.join(result['citations'])}\")\n",
    "        print(f\"STATS: {result['chunks_used']} chunks, {result['graph_entities']} graph entities\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916945c1-ec3c-4647-a988-86fda2ee27dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
