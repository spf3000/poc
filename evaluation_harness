{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2641216f-4195-45f5-9eac-cd913726a5ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiktoken'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Any\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mneo4j\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphDatabase\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'"
     ]
    }
   ],
   "source": [
    "### Cell 1: Setup and Imports\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env from home directory in AzureML\n",
    "home_env = Path.home() / '.env'\n",
    "if home_env.exists():\n",
    "    load_dotenv(home_env)\n",
    "else:\n",
    "    load_dotenv()  # fallback to local .env\n",
    "\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USER = os.getenv('NEO4J_USER')\n",
    "NEO4J_PASS = os.getenv('NEO4J_PASS')\n",
    "driver.close()\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS), max_connection_lifetime=30)\n",
    "openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Debug exhibit loading\n",
    "print(\"Checking exhibit paths...\")\n",
    "exhibit_dir = Path(\"data/exhibits\")\n",
    "print(f\"Exhibit dir exists: {exhibit_dir.exists()}\")\n",
    "print(f\"Files found: {list(exhibit_dir.glob('*.txt'))}\")\n",
    "print(\"✅ Evaluation harness initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46608d76-4199-4c3b-aa28-2ab1e7987d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 2: Load Test Questions\n",
    "\n",
    "# Load evaluation questions\n",
    "evaluation_df = pd.read_csv(\"data/Prosecutor_Evaluation_Matrix.csv\")\n",
    "test_questions = evaluation_df[\"Question\"].dropna().tolist()\n",
    "\n",
    "# Add some additional test questions for comprehensive evaluation\n",
    "additional_questions = [\n",
    "    \"What forensic evidence links Patrick Phelan to the crime scene?\",\n",
    "    \"What was Dr. Somers' relationship with the Phelan family?\",\n",
    "    \"What inconsistencies exist in witness statements?\",\n",
    "    \"What is the timeline of events on November 23, 2001?\",\n",
    "    \"What charges have been filed against Patrick Phelan?\"\n",
    "]\n",
    "\n",
    "all_questions = test_questions + additional_questions\n",
    "print(f\"✅ Loaded {len(all_questions)} evaluation questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e40739-4125-423d-a947-ec533dcd21a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 3: Token Tracking Utilities\n",
    "\n",
    "def count_tokens(text: str, model=\"gpt-4o-mini\") -> int:\n",
    "    \"\"\"Count tokens using tiktoken\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        return len(encoding.encode(text))\n",
    "    except:\n",
    "        # Fallback approximation: ~4 chars per token\n",
    "        return len(text) // 4\n",
    "\n",
    "def calculate_cost(input_tokens: int, output_tokens: int, model=\"gpt-4o-mini\") -> float:\n",
    "    \"\"\"Calculate API cost based on token usage\"\"\"\n",
    "    # GPT-4o-mini pricing (as of late 2024)\n",
    "    input_cost_per_1k = 0.000150  # $0.150 per 1K input tokens\n",
    "    output_cost_per_1k = 0.000600  # $0.600 per 1K output tokens\n",
    "\n",
    "    input_cost = (input_tokens / 1000) * input_cost_per_1k\n",
    "    output_cost = (output_tokens / 1000) * output_cost_per_1k\n",
    "\n",
    "    return input_cost + output_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db584efe-c769-4b14-849a-1d68e22800f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neo4j'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/tmp/ipykernel_401838/879460010.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mneo4j\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphDatabase\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neo4j'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neo4j'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Cell 4: Import Existing Methods\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Import functions from hybrid-graph-rag.ipynb\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# You'll need to copy the key functions here or import them\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhybrid_graph_rag.ipynb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed_query\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate query embedding - copy from hybrid-graph-rag.ipynb\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2482\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2480\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2482\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2486\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/IPython/core/magics/execution.py:741\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    740\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 741\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_execfile_ipy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3007\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[0;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[1;32m   3005\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[0;32m-> 3007\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3008\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m   3009\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/tmp/ipykernel_401838/879460010.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mneo4j\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphDatabase\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neo4j'"
     ]
    }
   ],
   "source": [
    "\n",
    "### Cell 4: Import Existing Methods\n",
    "\n",
    "# Import functions from hybrid-graph-rag.ipynb\n",
    "# You'll need to copy the key functions here or import them\n",
    "%run 'hybrid_graph_rag.ipynb'\n",
    "\n",
    "def embed_query(text: str):\n",
    "    \"\"\"Generate query embedding - copy from hybrid-graph-rag.ipynb\"\"\"\n",
    "    try:\n",
    "        model = SentenceTransformer(\"/Users/alandevlin/data/all-MiniLM-L6-v2\")\n",
    "    except:\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    return model.encode(text, show_progress_bar=False).tolist()\n",
    "\n",
    "def run_existing_method(question: str, method: str) -> Dict:\n",
    "    \"\"\"Run BM25, Vector, or Hybrid method from hybrid-graph-rag.ipynb\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call your existing answer_question function\n",
    "    answer = answer_question(question, method=method)\n",
    "\n",
    "    # Calculate token usage for the answer\n",
    "    answer_text = answer['answer']\n",
    "    output_tokens = count_tokens(answer_text)\n",
    "\n",
    "    # Estimate input tokens (you'd need to reconstruct the prompt)\n",
    "    # For now, approximate based on chunks used\n",
    "    estimated_input_tokens = answer['chunks_used'] * 100  # rough estimate\n",
    "\n",
    "    total_tokens = estimated_input_tokens + output_tokens\n",
    "    cost = calculate_cost(estimated_input_tokens, output_tokens)\n",
    "\n",
    "    result = {\n",
    "        \"method\": method,\n",
    "        \"answer\": answer_text,\n",
    "        \"response_time\": time.time() - start_time,\n",
    "        \"citations\": answer['citations'],\n",
    "        \"chunks_used\": answer['chunks_used'],\n",
    "        \"input_tokens\": estimated_input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"cost\": cost\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d550813-9440-44a9-b326-9d07e35f28b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 5: Method 4 - Graph-Only + OpenAI\n",
    "\n",
    "def format_graph_for_llm(entities: List[Dict], relationships: List[Dict]) -> str:\n",
    "    \"\"\"Format graph data for LLM consumption\"\"\"\n",
    "    lines = [\"ENTITIES:\"]\n",
    "    for entity in entities[:20]:  # Limit for token efficiency\n",
    "        props = entity.get('props', {})\n",
    "        entity_type = entity.get('types', ['Unknown'])[0] if entity.get('types') else 'Unknown'\n",
    "        lines.append(f\"- {entity_type}: {props.get('label', 'Unknown')} {props}\")\n",
    "\n",
    "    lines.append(\"\\nRELATIONSHIPS:\")\n",
    "    for rel in relationships[:30]:  # Limit for token efficiency\n",
    "        source = rel.get('source', {}).get('label', 'Unknown')\n",
    "        target = rel.get('target', {}).get('label', 'Unknown')\n",
    "        rel_type = rel.get('rel_type', 'RELATED_TO')\n",
    "        lines.append(f\"- {source} --{rel_type}--> {target}\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def graph_only_method(question: str) -> Dict:\n",
    "    \"\"\"Pure graph traversal + OpenAI reasoning\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    with driver.session(database=\"dpppoc\") as s:\n",
    "        # Find entities related to question keywords\n",
    "        entities_query = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE ANY(word IN split(toLower($question), ' ') WHERE toLower(n.label) CONTAINS word)\n",
    "           OR ANY(word IN split(toLower($question), ' ') WHERE word CONTAINS toLower(n.label))\n",
    "        RETURN DISTINCT labels(n) as types, n{.*} as props\n",
    "        LIMIT 20\n",
    "        \"\"\"\n",
    "        entities = s.run(entities_query, question=question).data()\n",
    "\n",
    "        # Get relationships between relevant entities\n",
    "        if entities:\n",
    "            labels = [e['props'].get('label', '') for e in entities if e['props'].get('label')]\n",
    "            if labels:\n",
    "                relationships_query = \"\"\"\n",
    "                MATCH (a)-[r]->(b)\n",
    "                WHERE a.label IN $labels OR b.label IN $labels\n",
    "                RETURN type(r) as rel_type, a{.*} as source, b{.*} as target\n",
    "                LIMIT 30\n",
    "                \"\"\"\n",
    "                relationships = s.run(relationships_query, labels=labels).data()\n",
    "            else:\n",
    "                relationships = []\n",
    "        else:\n",
    "            relationships = []\n",
    "\n",
    "    # Format graph data\n",
    "    graph_context = format_graph_for_llm(entities, relationships)\n",
    "\n",
    "    # Create prompt\n",
    "    system_prompt = \"\"\"You are analyzing a legal case using only graph/relationship data.\n",
    "Answer questions based solely on the entities and relationships provided.\n",
    "Be precise and cite specific entities when possible.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"GRAPH DATA:\n",
    "{graph_context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Answer based only on the graph relationships shown above.\"\"\"\n",
    "\n",
    "    # Calculate input tokens\n",
    "    input_text = system_prompt + user_prompt\n",
    "    input_tokens = count_tokens(input_text)\n",
    "\n",
    "    # Call OpenAI\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content\n",
    "        output_tokens = count_tokens(answer)\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        cost = calculate_cost(input_tokens, output_tokens)\n",
    "\n",
    "    except Exception as e:\n",
    "        answer = f\"Error: {str(e)}\"\n",
    "        output_tokens = 0\n",
    "        total_tokens = input_tokens\n",
    "        cost = 0\n",
    "\n",
    "    return {\n",
    "        \"method\": \"graph_only\",\n",
    "        \"answer\": answer,\n",
    "        \"response_time\": time.time() - start_time,\n",
    "        \"entities_found\": len(entities),\n",
    "        \"relationships_found\": len(relationships),\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"cost\": cost\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "327ce3a6-5df5-4383-b664-6124233a1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 6: Method 5 - Context Stuffing\n",
    "\n",
    "def context_stuffing_method(question: str) -> Dict:\n",
    "    \"\"\"Traditional approach: all exhibits → ChatGPT\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load ALL exhibit files\n",
    "    exhibit_dir = Path(\"data/exhibits\")\n",
    "    all_content = []\n",
    "\n",
    "    # Load exhibit files\n",
    "    for exhibit_file in sorted(exhibit_dir.glob(\"E*.txt\")):\n",
    "        try:\n",
    "            content = exhibit_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            all_content.append(f\"=== {exhibit_file.name} ===\\n{content}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {exhibit_file}: {e}\")\n",
    "\n",
    "    # Add charge brief if exists\n",
    "    charge_brief = Path(\"lantana_charge_bundle/Lantana_Charge_Brief.txt\")\n",
    "    if charge_brief.exists():\n",
    "        try:\n",
    "            content = charge_brief.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            all_content.append(f\"=== Lantana_Charge_Brief.txt ===\\n{content}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading charge brief: {e}\")\n",
    "\n",
    "    # Concatenate everything\n",
    "    full_context = \"\\n\\n\".join(all_content)\n",
    "\n",
    "    # Check token limits and truncate if necessary\n",
    "    max_context_tokens = 100000  # Leave room for question and response\n",
    "    context_tokens = count_tokens(full_context)\n",
    "\n",
    "    if context_tokens > max_context_tokens:\n",
    "        print(f\"Context too large ({context_tokens} tokens), truncating...\")\n",
    "        # Truncate to fit\n",
    "        chars_per_token = len(full_context) / context_tokens\n",
    "        max_chars = int(max_context_tokens * chars_per_token)\n",
    "        full_context = full_context[:max_chars] + \"\\n\\n[CONTENT TRUNCATED DUE TO LENGTH]\"\n",
    "\n",
    "    # Create prompt\n",
    "    system_prompt = \"\"\"You are a legal analyst assisting prosecutors.\n",
    "Answer questions based solely on the evidence provided.\n",
    "Cite specific exhibits where possible.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"EVIDENCE:\n",
    "{full_context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Answer based only on the evidence provided above.\"\"\"\n",
    "\n",
    "    # Calculate input tokens\n",
    "    input_tokens = count_tokens(system_prompt + user_prompt)\n",
    "\n",
    "    # Call OpenAI\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content\n",
    "        output_tokens = count_tokens(answer)\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        cost = calculate_cost(input_tokens, output_tokens)\n",
    "\n",
    "    except Exception as e:\n",
    "        answer = f\"Error: {str(e)}\"\n",
    "        output_tokens = 0\n",
    "        total_tokens = input_tokens\n",
    "        cost = 0\n",
    "\n",
    "    return {\n",
    "        \"method\": \"context_stuffing\",\n",
    "        \"answer\": answer,\n",
    "        \"response_time\": time.time() - start_time,\n",
    "        \"context_size\": len(full_context),\n",
    "        \"context_tokens\": context_tokens,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"cost\": cost\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42ea6562-3f17-4a55-aa12-1af2b633a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Graph Stuffing Method\n",
    "def get_full_graph():\n",
    "    \"\"\"Extract the complete knowledge graph (excluding chunks/exhibits).\"\"\"\n",
    "    with driver.session(database=\"dpppoc\") as s:\n",
    "        # Get all entities (excluding Chunk and Exhibit nodes)\n",
    "        entities_query = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE NOT n:Chunk AND NOT n:Exhibit\n",
    "        RETURN labels(n) as types, n{.*} as props\n",
    "        ORDER BY n.label\n",
    "        \"\"\"\n",
    "        entities = s.run(entities_query).data()\n",
    "\n",
    "        # Get all relationships between entities\n",
    "        relationships_query = \"\"\"\n",
    "        MATCH (a)-[r]->(b)\n",
    "        WHERE NOT a:Chunk AND NOT a:Exhibit AND NOT b:Chunk AND NOT b:Exhibit\n",
    "        RETURN type(r) as rel_type, a{.*} as source, b{.*} as target\n",
    "        \"\"\"\n",
    "        relationships = s.run(relationships_query).data()\n",
    "\n",
    "        return entities, relationships\n",
    "\n",
    "def graph_stuffing_method(question: str) -> Dict:\n",
    "    \"\"\"Answer using the complete knowledge graph without text retrieval.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Get the entire graph structure\n",
    "    entities, relationships = get_full_graph()\n",
    "\n",
    "    # Format graph data using existing function\n",
    "    graph_context = format_graph_for_llm(entities, relationships)\n",
    "\n",
    "    # Create prompt\n",
    "    system_prompt = \"\"\"You are analyzing a legal case using complete knowledge graph data.\n",
    "Answer questions based solely on the entities and relationships provided from the entire graph.\n",
    "Be precise and cite specific entities when possible.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"COMPLETE GRAPH DATA:\n",
    "{graph_context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Answer based on the complete graph structure shown above.\"\"\"\n",
    "\n",
    "    # Calculate input tokens\n",
    "    input_text = system_prompt + user_prompt\n",
    "    input_tokens = count_tokens(input_text)\n",
    "\n",
    "    # Call OpenAI\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content\n",
    "        output_tokens = count_tokens(answer)\n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        cost = calculate_cost(input_tokens, output_tokens)\n",
    "\n",
    "    except Exception as e:\n",
    "        answer = f\"Error: {str(e)}\"\n",
    "        output_tokens = 0\n",
    "        total_tokens = input_tokens\n",
    "        cost = 0\n",
    "\n",
    "    return {\n",
    "        \"method\": \"graph_stuffing\",\n",
    "        \"answer\": answer,\n",
    "        \"response_time\": time.time() - start_time,\n",
    "        \"entities_found\": len(entities),\n",
    "        \"relationships_found\": len(relationships),\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"cost\": cost\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9624e0f0-f854-4831-bae7-e43c75d03b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell 8: Evaluation Runner\n",
    "\n",
    "def run_comprehensive_evaluation(questions: List[str], run_number: int = 0, max_questions: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Run all 5 methods on test questions\"\"\"\n",
    "    if max_questions:\n",
    "        questions = questions[:max_questions]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Question {i+1}/{len(questions)}: {question[:80]}...\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Method 1: BM25 (placeholder - replace with actual call)\n",
    "        print(\"Running BM25...\")\n",
    "        try:\n",
    "            result = run_existing_method(question, \"bm25\")\n",
    "            results.append({\"question\": question, **result})\n",
    "        except Exception as e:\n",
    "            print(f\"BM25 failed: {e}\")\n",
    "\n",
    "        # Method 2: Vector (placeholder - replace with actual call)\n",
    "        print(\"Running Vector...\")\n",
    "        try:\n",
    "            result = run_existing_method(question, \"vector\")\n",
    "            results.append({\"question\": question, **result})\n",
    "        except Exception as e:\n",
    "            print(f\"Vector failed: {e}\")\n",
    "\n",
    "        # Method 3: Hybrid (placeholder - replace with actual call)\n",
    "        print(\"Running Hybrid...\")\n",
    "        try:\n",
    "            result = run_existing_method(question, \"hybrid\")\n",
    "            results.append({\"question\": question, **result})\n",
    "        except Exception as e:\n",
    "            print(f\"Hybrid failed: {e}\")\n",
    "\n",
    "        # Method 4: Graph-Only\n",
    "        print(\"Running Graph-Only...\")\n",
    "        try:\n",
    "            result = graph_only_method(question)\n",
    "            results.append({\"question\": question, **result})\n",
    "            print(f\"  Found {result['entities_found']} entities, {result['relationships_found']} relationships\")\n",
    "        except Exception as e:\n",
    "            print(f\"Graph-only failed: {e}\")\n",
    "\n",
    "        # Method 5: Context Stuffing\n",
    "        print(\"Running Context Stuffing...\")\n",
    "        try:\n",
    "            result = context_stuffing_method(question)\n",
    "            results.append({\"question\": question, **result})\n",
    "            print(f\"  Context size: {result['context_tokens']} tokens\")\n",
    "        except Exception as e:\n",
    "            print(f\"Context stuffing failed: {e}\")\n",
    "\n",
    "        # Method 6: Graph Stuffing\n",
    "        print(\"Running Graph Stuffing...\")\n",
    "        try:\n",
    "            result = graph_stuffing_method(question)\n",
    "            results.append({\"question\": question, **result})\n",
    "        except Exception as e:\n",
    "            print(f\"Graph stuffing failed: {e}\")\n",
    "            \n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Save results\n",
    "    df.to_csv(f\"evaluation_results{run_number}.csv\", index=False)\n",
    "    print(f\"\\n✅ Evaluation complete! Saved {len(df)} results to evaluation_results{run_number}.csv\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e7f35e9-deff-4a55-8603-a22d992e1609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_retrieved_context(chunks, method):\n",
    "    \"\"\"Format retrieved chunks for CSV storage\"\"\"\n",
    "    if not chunks:\n",
    "        return \"[]\"\n",
    "\n",
    "    context_texts = []\n",
    "    for chunk in chunks[:8]:  # Limit to top 8 chunks\n",
    "        if isinstance(chunk, dict):\n",
    "            text = chunk.get('text', str(chunk))\n",
    "        else:\n",
    "            text = str(chunk)\n",
    "        # Truncate and clean for CSV storage\n",
    "        clean_text = text.replace('\\n', ' ').replace('\\r', ' ')[:500]\n",
    "        context_texts.append(clean_text)\n",
    "\n",
    "    return str(context_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b45e7c5-3ec1-4fe7-979c-64bed0d9dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_evaluation(questions: List[str], run_number: int = 0, max_questions: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Run all 6 methods on test questions with retrieved context saving\"\"\"\n",
    "    if max_questions:\n",
    "        questions = questions[:max_questions]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Question {i+1}/{len(questions)}: {question[:80]}...\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Method 1: BM25 with context saving\n",
    "        print(\"Running BM25...\")\n",
    "        try:\n",
    "            # Get both chunks and final result\n",
    "            chunks = retrieve_chunks(question, method=\"bm25\")\n",
    "            result = run_existing_method(question, \"bm25\")\n",
    "\n",
    "            # Add retrieved context\n",
    "            result[\"retrieved_context\"] = save_retrieved_context(chunks, \"bm25\")\n",
    "            result[\"context_chunks_count\"] = len(chunks)\n",
    "\n",
    "            results.append({\"question\": question, **result})\n",
    "        except Exception as e:\n",
    "            print(f\"BM25 failed: {e}\")\n",
    "\n",
    "        # Method 2: Vector with context saving\n",
    "        print(\"Running Vector...\")\n",
    "        try:\n",
    "            chunks = retrieve_chunks(question, method=\"vector\")\n",
    "            result = run_existing_method(question, \"vector\")\n",
    "\n",
    "            result[\"retrieved_context\"] = save_retrieved_context(chunks, \"vector\")\n",
    "            result[\"context_chunks_count\"] = len(chunks)\n",
    "\n",
    "            results.append({\"question\": question, **result})\n",
    "        except Exception as e:\n",
    "            print(f\"Vector failed: {e}\")\n",
    "\n",
    "        # Method 3: Hybrid with context saving\n",
    "        print(\"Running Hybrid...\")\n",
    "        try:\n",
    "            chunks = retrieve_chunks(question, method=\"hybrid\")\n",
    "            result = run_existing_method(question, \"hybrid\")\n",
    "\n",
    "            result[\"retrieved_context\"] = save_retrieved_context(chunks, \"hybrid\")\n",
    "            result[\"context_chunks_count\"] = len(chunks)\n",
    "\n",
    "            results.append({\"question\": question, **result})\n",
    "        except Exception as e:\n",
    "            print(f\"Hybrid failed: {e}\")\n",
    "\n",
    "        # Method 4: Graph-Only with context saving\n",
    "        print(\"Running Graph-Only...\")\n",
    "        try:\n",
    "            result = graph_only_method(question)\n",
    "\n",
    "            # Add graph context info\n",
    "            result[\"retrieved_context\"] = f\"Graph entities: {result['entities_found']}, relationships: {result['relationships_found']}\"\n",
    "            result[\"context_chunks_count\"] = result['entities_found'] + result['relationships_found']\n",
    "\n",
    "            results.append({\"question\": question, **result})\n",
    "            print(f\"  Found {result['entities_found']} entities, {result['relationships_found']} relationships\")\n",
    "        except Exception as e:\n",
    "            print(f\"Graph-only failed: {e}\")\n",
    "\n",
    "        # Method 5: Context Stuffing with context saving\n",
    "        print(\"Running Context Stuffing...\")\n",
    "        try:\n",
    "            result = context_stuffing_method(question)\n",
    "\n",
    "            # Add context info for context stuffing\n",
    "            exhibit_dir = Path(\"exhibits\")\n",
    "            all_files = [f.name for f in exhibit_dir.glob(\"E*.txt\")]\n",
    "            result[\"retrieved_context\"] = f\"All exhibits: {all_files}\"\n",
    "            result[\"context_chunks_count\"] = len(all_files)\n",
    "\n",
    "            results.append({\"question\": question, **result})\n",
    "            print(f\"  Context size: {result.get('context_tokens', 'N/A')} tokens\")\n",
    "        except Exception as e:\n",
    "            print(f\"Context stuffing failed: {e}\")\n",
    "\n",
    "        # Method 6: Graph Stuffing with context saving\n",
    "        print(\"Running Graph Stuffing...\")\n",
    "        try:\n",
    "            result = graph_stuffing_method(question)\n",
    "\n",
    "            # Add full graph context info\n",
    "            result[\"retrieved_context\"] = f\"Full graph: {result['entities_found']} entities, {result['relationships_found']} relationships\"\n",
    "            result[\"context_chunks_count\"] = result['entities_found'] + result['relationships_found']\n",
    "\n",
    "            results.append({\"question\": question, **result})\n",
    "        except Exception as e:\n",
    "            print(f\"Graph stuffing failed: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Save results with new context columns\n",
    "    df.to_csv(f\"evaluation_results{run_number}.csv\", index=False)\n",
    "    print(f\"\\n✅ Evaluation complete! Saved {len(df)} results to evaluation_results{run_number}.csv\")\n",
    "    print(f\"New columns added: 'retrieved_context', 'context_chunks_count'\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Helper function to save retrieved context\n",
    "def save_retrieved_context(chunks, method):\n",
    "    \"\"\"Format retrieved chunks for CSV storage\"\"\"\n",
    "    if not chunks:\n",
    "        return \"[]\"\n",
    "\n",
    "    context_texts = []\n",
    "    for chunk in chunks[:8]:  # Limit to top 8 chunks\n",
    "        if isinstance(chunk, dict):\n",
    "            text = chunk.get('text', str(chunk))\n",
    "        else:\n",
    "            text = str(chunk)\n",
    "        # Truncate and clean for CSV storage\n",
    "        clean_text = text.replace('\\n', ' ').replace('\\r', ' ')[:500]\n",
    "        context_texts.append(clean_text)\n",
    "\n",
    "    return str(context_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a0f227c-9c44-48ad-a249-a5ca4bf1f4b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Cell 9: Run Evaluation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m      3\u001b[0m driver \u001b[38;5;241m=\u001b[39m GraphDatabase\u001b[38;5;241m.\u001b[39mdriver(NEO4J_URI, auth\u001b[38;5;241m=\u001b[39m(NEO4J_USER, NEO4J_PASS), max_connection_lifetime\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Run evaluation on first n questions for testing\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "### Cell 9: Run Evaluation\n",
    "driver.close()\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS), max_connection_lifetime=30)\n",
    "# Run evaluation on first n questions for testing\n",
    "print(\"Starting evaluation...\")\n",
    "results_df = run_comprehensive_evaluation(all_questions, run_number = 1)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nEVALUATION SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "summary = results_df.groupby('method').agg({\n",
    "    'total_tokens': ['mean', 'sum'],\n",
    "    'cost': ['mean', 'sum'],\n",
    "    'response_time': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f0d8af7-3794-4b4c-b1a6-eecd28248e0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m### Cell 9: Analysis and Visualization\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Analyze results\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manalyze_results\u001b[39m(df: pd.DataFrame):\n\u001b[32m      7\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate comparative analysis\"\"\"\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "\n",
    "### Cell 9: Analysis and Visualization\n",
    "\n",
    "# Analyze results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_results(df: pd.DataFrame):\n",
    "    \"\"\"Generate comparative analysis\"\"\"\n",
    "\n",
    "    print(\"TOKEN USAGE COMPARISON:\")\n",
    "    print(\"-\" * 30)\n",
    "    token_summary = df.groupby('method')['total_tokens'].agg(['mean', 'sum', 'std']).round(2)\n",
    "    print(token_summary)\n",
    "\n",
    "    print(\"\\nCOST COMPARISON:\")\n",
    "    print(\"-\" * 20)\n",
    "    cost_summary = df.groupby('method')['cost'].agg(['mean', 'sum']).round(4)\n",
    "    print(cost_summary)\n",
    "\n",
    "    print(\"\\nRESPONSE TIME COMPARISON:\")\n",
    "    print(\"-\" * 25)\n",
    "    time_summary = df.groupby('method')['response_time'].agg(['mean', 'std']).round(3)\n",
    "    print(time_summary)\n",
    "\n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Token usage\n",
    "    df.groupby('method')['total_tokens'].mean().plot(kind='bar', ax=axes[0,0], title='Average Token Usage')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Cost comparison\n",
    "    df.groupby('method')['cost'].sum().plot(kind='bar', ax=axes[0,1], title='Total Cost')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Response time\n",
    "    df.groupby('method')['response_time'].mean().plot(kind='bar', ax=axes[1,0], title='Average Response Time (s)')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Token efficiency (tokens per question)\n",
    "    efficiency = df.groupby('method')['total_tokens'].mean()\n",
    "    efficiency.plot(kind='bar', ax=axes[1,1], title='Token Efficiency (avg tokens/question)')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('evaluation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Run analysis\n",
    "analyze_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd61a8b-7dbb-445e-8119-1b370eac065d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
