{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vector_overview",
   "metadata": {},
   "source": [
    "# Vector Embeddings Setup for DPP PoC\n",
    "\n",
    "This notebook handles:\n",
    "- Text chunking from exhibits\n",
    "- Vector embedding generation\n",
    "- Neo4j vector index creation\n",
    "- Chunk-to-exhibit relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a80e0de-ad70-4b0f-868c-496616550a38",
   "metadata": {},
   "source": [
    "# Vector Embeddings Setup for DPP PoC\n",
    "\n",
    "This notebook handles:\n",
    "- Text chunking from exhibits\n",
    "- Vector embedding generation\n",
    "- Neo4j vector index creation\n",
    "- Chunk-to-exhibit relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9520e4f5-5b46-4191-9d47-34fc0a227d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.12 (main, Oct 21 2025, 02:11:22) [GCC 14.2.0]\n",
      "sentence-transformers version: 5.1.2\n",
      "Path.home(): /root\n",
      "Current working directory: /app/notebooks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "import sentence_transformers\n",
    "print(f\"sentence-transformers version: {sentence_transformers.__version__}\")\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "print(f\"Path.home(): {Path.home()}\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9c17323-542d-4634-a073-f73ab718690d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.12 (main, Oct 21 2025, 02:11:22) [GCC 14.2.0]\n",
      "sentence-transformers version: 5.1.2\n",
      "Path.home(): /root\n",
      "Current working directory: /app/notebooks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "import sentence_transformers\n",
    "print(f\"sentence-transformers version: {sentence_transformers.__version__}\")\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "print(f\"Path.home(): {Path.home()}\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44fe03a0-10a2-42c1-9ec4-733af421b295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.12 (main, Oct 21 2025, 02:11:22) [GCC 14.2.0]\n",
      "sentence-transformers version: 5.1.2\n",
      "Path.home(): /root\n",
      "Current working directory: /app/notebooks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "import sentence_transformers\n",
    "print(f\"sentence-transformers version: {sentence_transformers.__version__}\")\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "print(f\"Path.home(): {Path.home()}\")\n",
    "print(f\"Current working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "setup_imports",
   "metadata": {
    "gather": {
     "logged": 1761537694030
    }
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import pandas as pd, uuid, re, nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env from home directory in AzureML\n",
    "home_env = Path.home() / '.env'\n",
    "if home_env.exists():\n",
    "    load_dotenv(home_env)\n",
    "else:\n",
    "    load_dotenv()  # fallback to local .env\n",
    "\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USER = os.getenv('NEO4J_USER')\n",
    "NEO4J_PASS = os.getenv('NEO4J_PASS')\n",
    "\n",
    "EXHIBIT_DIR = Path(\"data/exhibits\")  # folder with E*.txt\n",
    "NODES_CSV   = \"data/lantana_charge_bundle/POLE_nodes.csv\"      # already uploaded\n",
    "\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"   # <-- change here if you switch models\n",
    "EMBED_DIM   = 384                  # <-- must match your Neo4j vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "chunking_function",
   "metadata": {
    "gather": {
     "logged": 1761530798232
    }
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, max_chars=400):\n",
    "    \"\"\"Split text into chunks, handling legal document structure.\"\"\"\n",
    "    # First try splitting by logical sections (A., B., C., Summary:, etc.)\n",
    "    section_splits = re.split(r'\\n(?=[A-Z]\\.|Summary:|Remarks:|\\d+\\.)', text)\n",
    "    \n",
    "    if len(section_splits) > 1:\n",
    "        # We found logical sections, process each\n",
    "        for section in section_splits:\n",
    "            section = section.strip()\n",
    "            if not section:\n",
    "                continue\n",
    "            if len(section) <= max_chars:\n",
    "                yield section\n",
    "            else:\n",
    "                # Section too long, split by sentences\n",
    "                sentences = re.split(r'(?<=\\.)\\s+', section)\n",
    "                buf = []\n",
    "                for s in sentences:\n",
    "                    s = s.strip()\n",
    "                    if not s:\n",
    "                        continue\n",
    "                    if sum(len(x) for x in buf) + len(s) + 1 <= max_chars:\n",
    "                        buf.append(s)\n",
    "                    else:\n",
    "                        if buf:\n",
    "                            yield ' '.join(buf)\n",
    "                        buf = [s]\n",
    "                if buf:\n",
    "                    yield ' '.join(buf)\n",
    "    else:\n",
    "        # No logical sections found, fall back to paragraph/sentence splitting\n",
    "        paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "        if len(paras) <= 1:\n",
    "            # No paragraph breaks, split by lines or sentences\n",
    "            lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "            buf = []\n",
    "            for line in lines:\n",
    "                if sum(len(x) for x in buf) + len(line) + 1 <= max_chars:\n",
    "                    buf.append(line)\n",
    "                else:\n",
    "                    if buf:\n",
    "                        yield ' '.join(buf)\n",
    "                    buf = [line]\n",
    "            if buf:\n",
    "                yield ' '.join(buf)\n",
    "        else:\n",
    "            # Process paragraphs normally\n",
    "            for para in paras:\n",
    "                if len(para) <= max_chars:\n",
    "                    yield para\n",
    "                else:\n",
    "                    sentences = re.split(r'(?<=\\.)\\s+', para)\n",
    "                    buf = []\n",
    "                    for s in sentences:\n",
    "                        s = s.strip()\n",
    "                        if not s:\n",
    "                            continue\n",
    "                        if sum(len(x) for x in buf) + len(s) + 1 <= max_chars:\n",
    "                            buf.append(s)\n",
    "                        else:\n",
    "                            if buf:\n",
    "                                yield ' '.join(buf)\n",
    "                            buf = [s]\n",
    "                    if buf:\n",
    "                        yield ' '.join(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "embedding_setup",
   "metadata": {
    "gather": {
     "logged": 1761531441503
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "✅ Downloaded model from HuggingFace\n",
      "Loading entity labels...\n",
      "✅ Loaded 32 entity labels for mention linking\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model - use model name for AzureML compatibility\n",
    "print(\"Loading embedding model...\")\n",
    "  # Fallback to download (for AzureML)\n",
    "model = SentenceTransformer(EMBED_MODEL)\n",
    "print(\"✅ Downloaded model from HuggingFace\")\n",
    "\n",
    "def embed(text): \n",
    "    \"\"\"Embed text using the loaded model.\"\"\"\n",
    "    return model.encode(text, show_progress_bar=False).tolist()\n",
    "\n",
    "# Load nodes for mention linking\n",
    "print(\"Loading entity labels...\")\n",
    "nodes = pd.read_csv(NODES_CSV)\n",
    "labels = sorted(set(nodes[\"label\"].dropna().astype(str)))\n",
    "print(f\"✅ Loaded {len(labels)} entity labels for mention linking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cleanup_chunks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up existing chunks...\n",
      "neo: bolt://20.58.149.213:7687\n",
      "✅ Deleted 20 existing chunks\n",
      "✅ Dropped existing vector index\n",
      "Ready to rebuild chunks with improved chunking strategy.\n"
     ]
    }
   ],
   "source": [
    "# CLEANUP: Remove existing chunks to rebuild with better chunking\n",
    "print(\"Cleaning up existing chunks...\")\n",
    "print(f\"neo: {NEO4J_URI}\")\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "\n",
    "with driver.session(database=\"dpppoc\") as s:\n",
    "    # Remove all chunks and their relationships\n",
    "    result = s.run(\"MATCH (c:Chunk) DETACH DELETE c RETURN count(*) as deleted\")\n",
    "    deleted = result.single()[\"deleted\"]\n",
    "    print(f\"✅ Deleted {deleted} existing chunks\")\n",
    "    \n",
    "    # Also remove the vector index to recreate it fresh\n",
    "    try:\n",
    "        s.run(\"DROP INDEX chunk_vec IF EXISTS\")\n",
    "        print(\"✅ Dropped existing vector index\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: {e}\")\n",
    "\n",
    "driver.close()\n",
    "print(\"Ready to rebuild chunks with improved chunking strategy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "create_chunks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector embeddings and chunks...\n",
      "Found 9 exhibit files\n",
      "Setting up database constraints and indexes...\n",
      "Processing E0_Charge_Brief.txt (1/9)...\n",
      "  Creating 19 chunks...\n",
      "    Chunk 5/19\n",
      "    Chunk 10/19\n",
      "    Chunk 15/19\n",
      "  ✅ Completed E0_Charge_Brief.txt\n",
      "Processing E1_Forensic_Scene_Photographs.txt (2/9)...\n",
      "  Creating 2 chunks...\n",
      "  ✅ Completed E1_Forensic_Scene_Photographs.txt\n",
      "Processing E2_Autopsy_Report.txt (3/9)...\n",
      "  Creating 3 chunks...\n",
      "  ✅ Completed E2_Autopsy_Report.txt\n",
      "Processing E3_Mobile_Phone_Tower_Log.txt (4/9)...\n",
      "  Creating 2 chunks...\n",
      "  ✅ Completed E3_Mobile_Phone_Tower_Log.txt\n",
      "Processing E4_Vehicle_Examination_Report.txt (5/9)...\n",
      "  Creating 2 chunks...\n",
      "  ✅ Completed E4_Vehicle_Examination_Report.txt\n",
      "Processing E5_Interview_Patrick_Phelan.txt (6/9)...\n",
      "  Creating 3 chunks...\n",
      "  ✅ Completed E5_Interview_Patrick_Phelan.txt\n",
      "Processing E6_Therapy_Notes_and_Fibre_Analysis.txt (7/9)...\n",
      "  Creating 2 chunks...\n",
      "  ✅ Completed E6_Therapy_Notes_and_Fibre_Analysis.txt\n",
      "Processing E7_Witness_Statements.txt (8/9)...\n",
      "  Creating 4 chunks...\n",
      "  ✅ Completed E7_Witness_Statements.txt\n",
      "Processing E8_Toll_Records_and_Surveillance.txt (9/9)...\n",
      "  Creating 2 chunks...\n",
      "  ✅ Completed E8_Toll_Records_and_Surveillance.txt\n",
      "✅ Created 39 chunks from 9 exhibits\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating vector embeddings and chunks...\")\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "\n",
    "# Check if exhibits exist first\n",
    "exhibit_files = list(EXHIBIT_DIR.glob(\"E*.txt\"))\n",
    "print(f\"Found {len(exhibit_files)} exhibit files\")\n",
    "\n",
    "if not exhibit_files:\n",
    "    print(\"⚠️  No exhibit files found. Check EXHIBIT_DIR path.\")\n",
    "else:\n",
    "    with driver.session(database=\"dpppoc\") as s:\n",
    "        # Ensure constraints/index exist (safe to run repeatedly)\n",
    "        print(\"Setting up database constraints and indexes...\")\n",
    "        s.run(\"CREATE CONSTRAINT exhibit_id IF NOT EXISTS FOR (e:Exhibit) REQUIRE e.id IS UNIQUE\")\n",
    "        s.run(\"CREATE CONSTRAINT chunk_id   IF NOT EXISTS FOR (c:Chunk)   REQUIRE c.id IS UNIQUE\")\n",
    "        s.run(\"\"\"\n",
    "        CREATE VECTOR INDEX chunk_vec IF NOT EXISTS FOR (c:Chunk) ON (c.embedding)\n",
    "        OPTIONS { indexConfig: { `vector.dimensions`: $dim, `vector.similarity_function`: \"cosine\" } }\n",
    "        \"\"\", dim=EMBED_DIM)\n",
    "\n",
    "        total_chunks = 0\n",
    "        for i, p in enumerate(sorted(exhibit_files), 1):\n",
    "            print(f\"Processing {p.name} ({i}/{len(exhibit_files)})...\")\n",
    "            \n",
    "            text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            m = re.match(r\"^(E\\d+)\", p.stem)\n",
    "            ex_id = m.group(1) if m else p.stem\n",
    "\n",
    "            # Upsert Exhibit\n",
    "            s.run(\"\"\"\n",
    "            MERGE (e:Exhibit {id:$id})\n",
    "            ON CREATE SET e.filename=$file\n",
    "            ON MATCH  SET e.filename=$file\n",
    "            \"\"\", id=ex_id, file=p.name)\n",
    "\n",
    "            # Process chunks for this exhibit\n",
    "            chunks = list(chunk_text(text))\n",
    "            print(f\"  Creating {len(chunks)} chunks...\")\n",
    "            \n",
    "            start = 0\n",
    "            for chunk_num, para in enumerate(chunks, 1):\n",
    "                if chunk_num % 5 == 0:  # Progress indicator\n",
    "                    print(f\"    Chunk {chunk_num}/{len(chunks)}\")\n",
    "                \n",
    "                cid = str(uuid.uuid4())\n",
    "                vec = embed(para)\n",
    "                end = start + len(para)\n",
    "\n",
    "                s.run(\"\"\"\n",
    "                MATCH (e:Exhibit {id:$ex})\n",
    "                CREATE (c:Chunk {\n",
    "                  id:$id, text:$text, exhibit_id:$ex,\n",
    "                  source_file:$file, offset_start:$start, offset_end:$end,\n",
    "                  embedding:$emb\n",
    "                })\n",
    "                MERGE (c)-[:FROM_EXHIBIT]->(e)\n",
    "                \"\"\", ex=ex_id, id=cid, text=para, file=p.name, start=start, end=end, emb=vec)\n",
    "                start = end + 2\n",
    "            \n",
    "            total_chunks += len(chunks)\n",
    "            print(f\"  ✅ Completed {p.name}\")\n",
    "\n",
    "    print(f\"✅ Created {total_chunks} chunks from {len(exhibit_files)} exhibits\")\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "create_mentions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mention relationships...\n",
      "Processing entities 1-20 of 32...\n",
      "Processing entities 21-32 of 32...\n",
      "✅ Mention relationships created\n"
     ]
    }
   ],
   "source": [
    "# Create mention relationships between chunks and entities\n",
    "print(\"Creating mention relationships...\")\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "\n",
    "with driver.session(database=\"dpppoc\") as s:\n",
    "    # Process in smaller batches to avoid timeout\n",
    "    batch_size = 20\n",
    "    for i in range(0, len(labels), batch_size):\n",
    "        batch_labels = labels[i:i+batch_size]\n",
    "        print(f\"Processing entities {i+1}-{min(i+batch_size, len(labels))} of {len(labels)}...\")\n",
    "        \n",
    "        s.run(\"\"\"\n",
    "        WITH $labels AS labels\n",
    "        UNWIND labels AS name\n",
    "        MATCH (c:Chunk)\n",
    "        WHERE toLower(c.text) CONTAINS toLower(name)\n",
    "        OPTIONAL MATCH (p:Person   {label:name})\n",
    "        OPTIONAL MATCH (o:Object   {label:name})\n",
    "        OPTIONAL MATCH (l:Location {label:name})\n",
    "        OPTIONAL MATCH (e:Event    {label:name})\n",
    "        FOREACH (_ IN CASE WHEN p IS NULL THEN [] ELSE [1] END | MERGE (c)-[:MENTIONS]->(p))\n",
    "        FOREACH (_ IN CASE WHEN o IS NULL THEN [] ELSE [1] END | MERGE (c)-[:MENTIONS]->(o))\n",
    "        FOREACH (_ IN CASE WHEN l IS NULL THEN [] ELSE [1] END | MERGE (c)-[:MENTIONS]->(l))\n",
    "        FOREACH (_ IN CASE WHEN e IS NULL THEN [] ELSE [1] END | MERGE (c)-[:MENTIONS]->(e))\n",
    "        \"\"\", labels=batch_labels, timeout=30000)  # 30 second timeout\n",
    "    \n",
    "    print(\"✅ Mention relationships created\")\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "test_vector_search",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing vector search...\n",
      "Query: Who was the last person to see the victim alive?\n",
      "Found 39 chunks in database\n",
      "\n",
      "Vector search test results:\n",
      "E0 | score=0.748\n",
      "Somers died as a result of blunt force trauma to the head consistent with an assault. Based on the totality of evidence obtained, Patrick Phelan, male, 39 years, is charged with Murder (Section 18, Cr...\n",
      "---\n",
      "\n",
      "E0 | score=0.746\n",
      "1. Executive Summary\n",
      "This brief of evidence concerns the unlawful killing of Dr. Valerie Somers, psychologist, aged 42 years, whose body was recovered from bushland near Lantana Park, Sydney, on 2 Dec...\n",
      "---\n",
      "\n",
      "E2 | score=0.722\n",
      "Exhibit E2 – Autopsy Report\n",
      "Victim: Dr. Valerie Somers\n",
      "Examiner: Dr. Elaine Wong, NSW Forensic Pathology\n",
      "Date: 3 December 2001...\n",
      "---\n",
      "\n",
      "E0 | score=0.719\n",
      "2. Persons Involved\n",
      "Victim: Dr. Valerie Somers – Clinical psychologist; resident of Lane Cove; estranged from husband John Knox. Accused: Patrick Phelan – 39; unemployed; neighbour of John Knox; prior...\n",
      "---\n",
      "\n",
      "E5 | score=0.711\n",
      "Summary:\n",
      "- Initially denies contact with victim on 23 November.\n",
      "- When confronted with forensic findings, admits to meeting Dr. Somers “to settle differences.”\n",
      "- States an argument escalated, claims s...\n",
      "---\n",
      "\n",
      "✅ Vector search working! Found 5 results\n",
      "✅ Vector embeddings setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Test vector search functionality\n",
    "print(\"Testing vector search...\")\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))\n",
    "\n",
    "# Reuse the existing model instead of reloading\n",
    "question = \"Who was the last person to see the victim alive?\"\n",
    "print(f\"Query: {question}\")\n",
    "\n",
    "qvec = embed(question)  # Use the embed function we already defined\n",
    "\n",
    "with driver.session(database=\"dpppoc\") as s:\n",
    "    # First check if we have any chunks\n",
    "    chunk_count = s.run(\"MATCH (c:Chunk) RETURN count(c) AS count\").single()[\"count\"]\n",
    "    print(f\"Found {chunk_count} chunks in database\")\n",
    "    \n",
    "    if chunk_count > 0:\n",
    "        result = s.run(\"\"\"\n",
    "            CALL db.index.vector.queryNodes('chunk_vec', 5, $embedding)\n",
    "            YIELD node AS c, score\n",
    "            RETURN c.exhibit_id AS exhibit, c.source_file AS file, c.text AS snippet, score\n",
    "            ORDER BY score DESC\n",
    "            LIMIT 5\n",
    "        \"\"\", embedding=qvec)\n",
    "        \n",
    "        print(\"\\nVector search test results:\")\n",
    "        results = list(result)\n",
    "        if results:\n",
    "            for row in results:\n",
    "                print(f\"{row['exhibit']} | score={row['score']:.3f}\")\n",
    "                print(f\"{row['snippet'][:200]}...\\n---\\n\")\n",
    "            print(f\"✅ Vector search working! Found {len(results)} results\")\n",
    "        else:\n",
    "            print(\"⚠️  No vector search results - check if vector index is populated\")\n",
    "    else:\n",
    "        print(\"⚠️  No chunks found - run the embedding creation cells first\")\n",
    "\n",
    "driver.close()\n",
    "print(\"✅ Vector embeddings setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d9d5b0-cff6-4812-b8b8-c0dc1d2a8808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "graphrag"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
